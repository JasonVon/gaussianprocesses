## Table of Content

* [Presenters](#presenters)
* [Abstract](#abstract)
* [Link to Slides](#link-to-slides)
* [Notebooks](#notebooks)
* [References](#references)


## Presenters


## Abstract

Since the celebrated book by Rasmussen and Williams, there have been a considerable amount of novel contributions that are allowing the applicability of Gaussian processes (GPs) to problems at an unprecedented scale and to new areas where uncertainty quantification is of fundamental importance. 
This tutorial will expose attendees to recent advances in GP research; describe the current challenges in modeling and inference with GPs; their relationship to neural networks and deep neural networks and stimulate the debate about the role of GP models in solving complex machine-learning tasks. 

## Link to Slides

* [Introduction](slides/introduction.pdf)
* [Definition of Gaussian Processes](slides/gaussian_processes.pdf)
* [Model Approximations](slides/model_approximations.pdf)
* [Inference](slides/inference.pdf)
* [Challenges](slides/challenges.pdf)
* [Theory and Code](slides/theory_code.pdf)




## Notebooks

[Notebook Sampling from GP prior](notebooks/gp-priors.ipynb)

[Notebook on GP Regression](notebooks/gp-inference.ipynb)


## References

<link rel="import" href="references/references.html">
### Bayesian Deep Nets and Deep Gaussian Processes
<ul>



<li><a name="Matthews18"></a>

De, J.&nbsp;Hron, M.&nbsp;Rowland, R.&nbsp;E. Turner, and Z.&nbsp;Ghahramani.
 Gaussian Process Behaviour in Wide Deep Neural Networks.
 In <em>International Conference on Learning Representations</em>, 2018.

</li>

<li><a name="Cutajar17"></a>

K.&nbsp;Cutajar, E.&nbsp;V. Bonilla, P.&nbsp;Michiardi, and M.&nbsp;Filippone.
 Random feature expansions for deep Gaussian processes.
 In D.&nbsp;Precup and Y.&nbsp;W. Teh, editors, <em>Proceedings of the 34th
  International Conference on Machine Learning</em>, volume&nbsp;70 of <em>Proceedings
  of Machine Learning Research</em>, pages 884--893, International Convention
  Centre, Sydney, Australia, Aug. 2017. PMLR.

</li>

<li><a name="Gal16"></a>

Y.&nbsp;Gal and Z.&nbsp;Ghahramani.
 Dropout As a Bayesian Approximation: Representing Model Uncertainty
  in Deep Learning.
 In <em>Proceedings of the 33rd International Conference on
  International Conference on Machine Learning - Volume 48</em>, ICML'16, pages
  1050--1059. JMLR.org, 2016.

</li>

<li><a name="Duvenaud14"></a>

D.&nbsp;K. Duvenaud, O.&nbsp;Rippel, R.&nbsp;P. Adams, and Z.&nbsp;Ghahramani.
 Avoiding pathologies in very deep networks.
 In <em>Proceedings of the Seventeenth International Conference on
  Artificial Intelligence and Statistics, AISTATS 2014, Reykjavik, Iceland,
  April 22-25, 2014</em>, volume&nbsp;33 of <em>JMLR Workshop and Conference
  Proceedings</em>, pages 202--210. JMLR.org, 2014.

</li>

<li><a name="Neal96"></a>

R.&nbsp;M. Neal.
 <em>Bayesian Learning for Neural Networks (Lecture Notes in
  Statistics)</em>.
 Springer, 1 edition, Aug. 1996.

</li></ul>
