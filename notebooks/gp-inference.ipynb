{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1 style=\"text-align:center\">Modern Gaussian Processes: Scalable Inference and Novel Applications (IJCNN '19)</h1>\n",
    "<h2 style=\"text-align:center\">Tutorial: Inference of Gaussian Process Regression models</h2>\n",
    "\n",
    "- **Edwin V. Bonilla**, Data61, Australia \n",
    "- **Maurizio Filippone**, EURECOM, France"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Aims\n",
    "<div class=\"alert alert-info\">\n",
    "<ul> \n",
    "<li> To implement Gaussian process inference for regression.\n",
    "<li> To use the above to observe samples from a Gaussian process posterior distribution.\n",
    "<li> To evaluate how different hyperparameter settings impact model quality.\n",
    "<li> To investigate different kernel functions and parameter optimisation strategies.\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "**Note**: we shall use PyTorch for reasons that will be clear later on in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.spatial\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import warnings\n",
    "matplotlib.rc_file('matplotlibrc')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Introduction\n",
    "<div class=\"alert alert-info\">\n",
    "In this tutorial, we shall cover the basic concepts of <b>GP regression</b>. For the sake of clarity, we shall focus on univariate data, which allows for better visualisation of the GP model. Nonetheless, the code implemented within this lab can be very easily extended to handle\n",
    "multi-dimensional inputs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "As in the previous notebook, we shall consider a one-dimensional regression problem, whereby the inputs x are transformed by\n",
    "a function \n",
    "<br><br>\n",
    "    $$ f(\\mathbf{x}) = sin(exp(0.03 * \\mathbf{x}))\\,.$$\n",
    "<br>\n",
    "Let's generate 500 random points, $x$, in the range $[-20, 80]$, and compute their corresponding function\n",
    "values, $y$ (assuming noisyless observations for the moment). The target function can then be plotted accordingly.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "set_seed()\n",
    "matplotlib.rc_file('matplotlibrc')\n",
    "n = 100\n",
    "\n",
    "# Define our function\n",
    "f = lambda x: np.sin(np.exp(.03 * x))\n",
    "# Define our observation points\n",
    "x = np.sort(np.random.uniform(-20,80,n))\n",
    "# Define our target points\n",
    "y = f(x)\n",
    "\n",
    "# Plot \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y, '.', c='black', label='Observations')\n",
    "ax.plot(np.linspace(-30, 85, 1000), f(np.linspace(-30, 85, 1000)), alpha=0.6, label=r\"$f({x}) = sin(e^{0.03 {x}})$\" )\n",
    "ax.set_title(\"Regression problem\")\n",
    "ax.set_ylim(-1.5, 1.5)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "Recall that since GPs are non-parametric we need to define a prior distribution over functions (models),\n",
    "specified as a multivariate Gaussian distribution $p(f) = N (\\mu, \\Sigma)$.\n",
    "\n",
    "Without loss of generality, we shall assume a zero-mean GP prior, i.e. $\\mu = 0$. The covariance\n",
    "matrix of the distribution, $\\Sigma$, may then be computed by evaluating the covariance between the\n",
    "input points. For this tutorial, we consider the widely used squared-exponential (RBF)\n",
    "covariance.\n",
    "\n",
    "As a reminder, the RBF kernel is defined between two points as: \n",
    "\n",
    "$$k(x, x') = \\sigma_f^2 \\exp \\Big( -\\dfrac {(x-x')^2}{2l^2} \\Big). $$\n",
    "\n",
    "This kernel is parameterised by a lengthscale parameter $l$, and variance $\\sigma_f^2$ . Given that the true\n",
    "function may be assumed to be corrupted with noise, we can also add a noise parameter, $\\sigma_n^2$ , to\n",
    "the diagonal entries of the resulting kernel matrix, $K$, such that\n",
    "$$K_y = K + \\sigma_n^2I.$$\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def cdist(x1, x2):\n",
    "    \"\"\"\n",
    "    Compute distance between each pair of the two collections of input tensors.\n",
    "    see scipy.spatial.distance.cdist\n",
    "    \"\"\"\n",
    "    x1_norm = x1.pow(2).sum(dim=-1, keepdim=True)\n",
    "    x2_norm = x2.pow(2).sum(dim=-1, keepdim=True)\n",
    "    res = torch.addmm(x2_norm.transpose(-2, -1), x1, x2.transpose(-2, -1), alpha=-2).add_(x1_norm)\n",
    "    res = res.clamp_min_(1e-30).sqrt_()\n",
    "    return res\n",
    "\n",
    "def rbf_kernel(x1, x2, lengthscale, variance):\n",
    "    \"\"\"\n",
    "    Compute the RBF covariance matrix \n",
    "    \"\"\"\n",
    "    K = variance * torch.exp(-cdist(x1[...,None], x2[...,None])**2 / (2 * lengthscale**2))\n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Assuming a zero-mean prior, and using the kernel matrix constructed with `rbf_kernel()` for input points x, we can sample from the prior distribution using the numpy `multivariate_normal()` function.\n",
    "<br><br>\n",
    "For the time being, you can initialise the kernel parameters as follows:\n",
    "<br>\n",
    "- lengthscale = 10<br>\n",
    "- variance = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "lengthscale = 10\n",
    "variance = .1\n",
    "mu = np.zeros(1000)\n",
    "K = rbf_kernel(torch.from_numpy(np.linspace(-30, 85, 1000)), torch.from_numpy(np.linspace(-30, 85, 1000)), lengthscale, variance)\n",
    "repe = 30\n",
    "R = np.random.multivariate_normal(mu,K,repe)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(repe):\n",
    "    ax.plot(np.linspace(-30, 85, 1000), R[i,:], c='#ff7f00', alpha=0.5)\n",
    "\n",
    "ax.plot(np.linspace(-30, 85, 1000), mu, color=\"grey\", label=r'prior $\\mu$')   \n",
    "ax.fill_between(np.linspace(-30, 85, 1000),mu + np.sqrt(variance) * 2, mu - np.sqrt(variance) * 2,\n",
    "                color=\"grey\", alpha=0.2, label=r'prior $2\\sigma\\approx95\\%\\,CI$')\n",
    "ax.set_title('Sampling from the GP prior')\n",
    "ax.set_ylim(-1.5, 1.5)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3. GP Inference\n",
    "<div class=\"alert alert-info\">\n",
    "Suppose we can now observe 3 points at random from the input data; we would expect that with this additional knowledge, the functions drawn from the updated GP distribution would be constrained to pass through these points (or at least close if corrupted with noise). The combination of the prior and the likelihood of the observed data leads to the posterior distribution over functions.\n",
    "<br><br>\n",
    "Assign 3 points at random from $x$ (and their corresponding function values) to `obs_x` and `obs_t`\n",
    "respectively. For now we shall assume that all other $x$ values are unobserved.<br><br>\n",
    "\n",
    "You are encouraged to use the given initial configuration.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "A complete implementation of `gp_inference` is provided for evaluating the posterior GP mean and variance using the equations given in the tutorial.\n",
    "<br><br>\n",
    "<b>Note</b>: Matrix inversions can be both numerically troublesome and slow to compute. In this notebook, we shall avoid computing matrix inversions directly by instead considering Cholesky decompositions for solving linear systems. You are encouraged to read more about Cholesky decompositions for GPs by consulting Appendix A.4 of <a target=\"_blank\" href=\"http://www.gaussianprocess.org/gpml/\">Gaussian Processes for Machine Learning (Rasmussen and Williams, 2005)</a> - available online!<br><br>\n",
    "The complete pseudo-code for the following procedure is provided in Algorithm 2.1 from Chapter 2 of this same book.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gp_inference(obs_x, obs_t, x_new, params):\n",
    "    \n",
    "    # unpack params\n",
    "    lengthscale = params[0]\n",
    "    variance = params[1]\n",
    "    noise = params[2]\n",
    "    \n",
    "    N = obs_x.shape[0]\n",
    "    \n",
    "    # compute kernel\n",
    "    K = rbf_kernel(obs_x, obs_x, lengthscale,variance)\n",
    "    K_y = K + noise * torch.eye(obs_x.shape[0])\n",
    "     \n",
    "    '''\n",
    "    \n",
    "    When computing the posterior mean, we would like to avoid evaluating\n",
    "        \n",
    "                            alpha = (K_y)^-1 * obs_t\n",
    "    \n",
    "    directly. The Cholesky decomposition can be applied using the following procedure.\n",
    "    \n",
    "        -> Compute the lower triangular Cholesky decomposition of K_y (which we shall call K_chol)\n",
    "        -> Compute 'alpha' as:\n",
    "        \n",
    "                            alpha = K_chol.T \\ (K_chol \\ obs_t)\n",
    "                            \n",
    "           where the back-substitution operator can be evaluated using the 'trtrs' (solve_triangular) \n",
    "           function in pytorch. Make sure to set the function's upper' flag as appropriate.\n",
    "            \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # compute the Cholesky decomposition of K_y\n",
    "    K_chol = torch.cholesky(K_y)\n",
    "    \n",
    "    # compute alpha\n",
    "    alpha = torch.trtrs(torch.trtrs(obs_t.view(-1, 1), K_chol, upper=False)[0], K_chol.t(), upper=True)[0]\n",
    "    \n",
    "    # compute the covariance between the training and test data\n",
    "    K_obs_pred = rbf_kernel(obs_x, x_new, lengthscale, variance)\n",
    "    \n",
    "    # compute the covariance for the test data\n",
    "    K_pred = rbf_kernel(x_new, x_new, lengthscale, variance)\n",
    "    \n",
    "    # compute the posterior mean\n",
    "    posterior_m = torch.matmul(K_obs_pred.t(), alpha)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Similarly, when computing\n",
    "    \n",
    "                        v = (kern_obs)^-1 * kern_obs_pred\n",
    "                        \n",
    "    employ the Cholesky decomposition as outlined above.\n",
    "                        \n",
    "    '''\n",
    "    # compute the posterior variance\n",
    "    v = torch.trtrs(torch.trtrs(K_obs_pred, K_chol,upper=False)[0], K_chol.t(),upper=True)[0]\n",
    "    posterior_v = noise * torch.eye(len(x_new)) + K_pred - torch.matmul(K_obs_pred.t(), v)\n",
    "    \n",
    "    # compute the marginal log-likelihood\n",
    "    log_lik = -.5 * (torch.sum(torch.log(torch.abs(torch.diag(K_chol)))) +\n",
    "                     torch.sum(torch.trtrs((obs_t).view(-1, 1), K_chol,upper=False)[0]**2)) - N/2. * np.log(2 * np.pi)\n",
    "    \n",
    "    return posterior_m[...,0], posterior_v, log_lik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the GP inference. For plot convenience, let's take 1000 points on the real axis as $x_\\mathrm{new}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "numObs = 3\n",
    "\n",
    "lengthscale = 10\n",
    "variance = .1\n",
    "noise = 1e-5\n",
    "\n",
    "params = [lengthscale, variance, noise]\n",
    "\n",
    "obs_x = np.sort((np.random.choice(x, numObs, replace=False)))\n",
    "obs_t = f(obs_x)\n",
    "\n",
    "posterior_m, posterior_v, log_lik = gp_inference(torch.from_numpy(obs_x).float(), \n",
    "                                                 torch.from_numpy(obs_t).float(), \n",
    "                                                 torch.from_numpy(np.linspace(-30, 85, 1000)).float(), params)\n",
    "posterior_std = np.sqrt(np.diag(posterior_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4. Sampling from the  GP Posterior\n",
    "<br>\n",
    "<div class=\"alert alert-info\">\n",
    "Now that you have computed the posterior mean and variance, let's create a figure showing the true function. To this figure, we add the posterior mean and uncertainty (show two standard deviations) evaluated on the same $x$ values. Remember that the variance at each point is given by the diagonal of the covariance matrix.\n",
    "    Let's also plot 10 samples from the posterior GP after inference.\n",
    "<!-- Recall that the standard deviation is the square root of the variance. -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "repe = 10\n",
    "R2 = np.random.multivariate_normal(posterior_m, posterior_v, repe) \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.linspace(-30, 85, 1000), f(np.linspace(-30, 85, 1000)), alpha=0.75, label=r\"$f({x}) = sin(e^{0.03 {x}})$\" )\n",
    "\n",
    "for i in range(repe):\n",
    "    ax.plot(np.linspace(-30, 85, 1000),R2[i,:],c=\"#ff7f00\", alpha=0.5)\n",
    "ax.plot(obs_x,obs_t, '.', c='black', label='Observations')\n",
    "\n",
    "ax.plot(np.linspace(-30, 85, 1000), posterior_m.numpy(), color=\"grey\", label=r'posterior $\\mu$')  \n",
    "\n",
    "ax.fill_between(np.linspace(-30, 85, 1000), posterior_m.numpy() + posterior_std * 2,posterior_m.numpy() - posterior_std * 2,\n",
    "                color=\"grey\", alpha=0.2, label=r'posterior $2\\sigma\\approx95\\%\\,CI$')\n",
    "\n",
    "ax.set_title('Sampling from the GP posterior')\n",
    "ax.legend()\n",
    "ax.set_ylim(-1.5, 1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Try to increase the number of observations. What do you see?\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "fig, ((ax0, ax1), (ax2, ax3)) = plt.subplots(2, 2, sharex=True, sharey=True)\n",
    "\n",
    "def run_inference(numObs, ax):\n",
    "    obs_x = np.sort((np.random.choice(x, numObs, replace=False)))\n",
    "    obs_t = f(obs_x)\n",
    "    ax.plot(obs_x,obs_t, '.', c='black', label='Observations')\n",
    "\n",
    "    posterior_m, posterior_v, log_lik = gp_inference(torch.from_numpy(obs_x).float(), \n",
    "                                                     torch.from_numpy(obs_t).float(), \n",
    "                                                     torch.from_numpy(np.linspace(-30, 85, 1000)).float(), params) \n",
    "    posterior_std = np.sqrt(np.diag(posterior_v))\n",
    "\n",
    "\n",
    "    ax.plot(np.linspace(-30, 85, 1000), posterior_m.numpy(), color=\"grey\", label=r'posterior $\\mu$')  \n",
    "\n",
    "    ax.fill_between(np.linspace(-30, 85, 1000), posterior_m.numpy() + posterior_std*2,posterior_m.numpy() - posterior_std*2,\n",
    "                    color=\"grey\", alpha=0.2, label=r'posterior $2\\sigma$')\n",
    "    ax.set_title('%d observations' % numObs)\n",
    "    \n",
    "    \n",
    "    ax.set_ylim(-1.5, 1.5)\n",
    "    \n",
    "    \n",
    "run_inference(4, ax0)\n",
    "run_inference(10, ax1)\n",
    "run_inference(15, ax2)\n",
    "run_inference(35, ax3)\n",
    "    \n",
    "\n",
    "ax3.legend(loc='lower left')\n",
    "fig.suptitle('GP inference with increasing training points', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 4.1 Model evaluation</h2>\n",
    "<div class=\"alert alert-info\">\n",
    "Try to change the GP prior (for instance the lenghtscale of the RBF kernel) and run again the GP inference. What do you see?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "fig, ((ax0, ax1), (ax2, ax3)) = plt.subplots(2, 2, sharex=True, sharey=True)\n",
    "numObs =  15 \n",
    "obs_x = np.sort((np.random.choice(x, numObs, replace=False)))\n",
    "obs_t = f(obs_x)\n",
    "\n",
    "def run_inference(params, ax):\n",
    "    ax.plot(obs_x,obs_t, '.', c='black', label='Observations')\n",
    "\n",
    "    posterior_m, posterior_v, log_lik = gp_inference(torch.from_numpy(obs_x).float(), \n",
    "                                                     torch.from_numpy(obs_t).float(), \n",
    "                                                     torch.from_numpy(np.linspace(-30, 85, 1000)).float(), params) \n",
    "    posterior_std = np.sqrt(np.diag(posterior_v))\n",
    "\n",
    "    ax.plot(np.linspace(-30, 85, 1000), posterior_m.numpy(), color=\"grey\", label=r'posterior $\\mu$')  \n",
    "\n",
    "    ax.fill_between(np.linspace(-30, 85, 1000), posterior_m.numpy() + posterior_std*2,posterior_m.numpy() - posterior_std*2,\n",
    "                    color=\"grey\", alpha=0.2, label=r'posterior $2\\sigma$')\n",
    "    ax.set_title(r'$l=$%.0f, $\\sigma_f^2=$%.2f' % (params[0], params[1]))\n",
    "    \n",
    "    ax.text(80, 1.1, r'$p(Y|X, \\theta)$ = %.2f' % log_lik, horizontalalignment='right')\n",
    "    \n",
    "    ax.set_ylim(-1.5, 1.5)\n",
    "    \n",
    "# params = [lengthscale, variance, noise]\n",
    "params = [3, 0.01, 1e-3]\n",
    "run_inference(params, ax0)\n",
    "params = [3, 0.1, 1e-3]\n",
    "run_inference(params, ax1)\n",
    "params = [10, 0.01, 1e-3]\n",
    "run_inference(params, ax2)\n",
    "params = [10, 0.1, 1e-3]\n",
    "run_inference(params, ax3)\n",
    "\n",
    "ax3.legend(loc='lower left')\n",
    "fig.suptitle('Inference with different GP prior', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "As a measure of model quality, you should check the log marginal likelihood of the model (the higher, the better).\n",
    "<!-- To this end, complete the code provided in `gp inference()` to include the negative log likelihood term. -->\n",
    "<br><br>\n",
    "You could attempt a grid search over a range of parameter values in order to determine which configuration yields the best result<br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "numObs =  15 \n",
    "obs_x = np.sort((np.random.choice(x, numObs, replace=False)))\n",
    "obs_t = f(obs_x)\n",
    "\n",
    "l_points = np.linspace(7, 10, 50)\n",
    "logsigmaf_points = np.logspace(-2, 0, 50, base=10)\n",
    "log_lik_points = np.zeros([len(l_points), len(logsigmaf_points)])\n",
    "\n",
    "for i, logl in enumerate(l_points):\n",
    "    for j, logsigmaf in enumerate(logsigmaf_points):\n",
    "        _, _, log_lik = gp_inference(torch.from_numpy(obs_x).float(), \n",
    "                                     torch.from_numpy(obs_t).float(), \n",
    "                                     torch.from_numpy(np.linspace(-30, 85, 1)).float(), [logl, 10. ** logsigmaf, 1e-5]) \n",
    "        log_lik_points[i, j] = -log_lik\n",
    "    \n",
    "fig, ax = plt.subplots()   \n",
    "cp = ax.contour(logsigmaf_points, l_points, log_lik_points, levels=np.logspace(np.log(6.5), np.log(9), 30, base=np.e))\n",
    "cb = plt.colorbar(cp)\n",
    "cb.ax.set_title(r\"$p(Y|X, \\theta)$\",)   \n",
    "best_coordinates = np.unravel_index(log_lik_points.argmin(), log_lik_points.shape)\n",
    "\n",
    "ax.plot(logsigmaf_points[best_coordinates[1]], l_points[best_coordinates[0]], '*', markersize=16, label='Best hyperparameters')\n",
    "ax.set_ylabel(r'lenghtscale l')\n",
    "ax.set_xlabel(r'signal variance $\\sigma_f^2$')\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_title('Optimization landscape\\nof the negative marginal likelihood')\n",
    "ax.semilogx()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But can we do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 5. Parameter Optimisation using Gradient Descent\n",
    "<br>\n",
    "<div class=\"alert alert-info\">\n",
    "Optimise the hyperparameters of the model by minimising the negative log-likelihood of the model. For a complete solution, you should include the derivatives of the objective function with respect to the parameters being optimised.\n",
    "<br><br>\n",
    "The general formula for computing the derivative is given below:<br>\n",
    "$$\n",
    "\\frac{\\partial\\;\\text{NLL}}{\\partial\\;\\theta_i} = - \\frac{1}{2} \\textbf{Tr} \\left( K^{-1} \\frac{\\partial K}{\\partial \\theta_i} \\right) + \\frac{1}{2} \\textbf{y}^{T} K^{-1} \\frac{\\partial K}{\\partial \\theta_i} K^{-1} \\textbf{y}.\n",
    "$$<br>\n",
    "To give a more concrete example, the $\\frac{\\partial K}{\\partial \\theta_i}$ term for the lengthscale parameter in the RBF kernel is computed as follows:\n",
    "$$\n",
    "\\frac{\\partial K}{\\partial l} = \\sigma_f^2 \\exp \\left( -\\dfrac {(x-x')^2}{2l^2} \\right)\\left( \\dfrac {(x-x')^2}{l^3} \\right)\n",
    "$$\n",
    "<br><br>\n",
    "<b>Pro tip:</b> Note that the parameters $l$, $\\sigma_f^2$ , and $\\sigma_n^2$ are always expected to be positive. It is possible that the optimisation algorithm attempts to evaluate the log-likelihood in regions of the parameter space where one or more of these parameters are negative, leading to numerical issues. A commonly-used technique to enforce this condition is to work with a transformed version of covariance parameters using the logarithm transformation. In particular, define $\\psi_l = log(l)$, $\\psi_f = log(\\sigma_f^2 )$, and $\\psi_n = log(\\sigma_n^2 )$, and optimise with respect to the $\\psi$ parameters. The optimisation problem in the transformed space is now unbounded, and the gradient of the log-likelihood should be computed with respect to the $\\psi$ parameters.\n",
    "    <br><br>\n",
    "    <b>Pro tip 2019:</b>\n",
    "    We don't really need to derive the gradients of the marginal likelihood w.r.t. parameters by hand. We can leverage the automatic differentiation engine in PyTorch! All the operations that we used (cdist and the RBF kernel function, the Cholesky decomposition and triangular linear system solver are all differentiable). Let's take advantage of that!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "# Transformed covariance parameters\n",
    "logl = torch.nn.Parameter(torch.tensor(2).float())\n",
    "logsigmaf = torch.nn.Parameter(torch.tensor(-2).float())\n",
    "logsigman = torch.nn.Parameter(torch.tensor(-2).float())\n",
    "\n",
    "# Just like vanilla PyTorch for DL\n",
    "optimizer = torch.optim.SGD([logl, logsigmaf, logsigman], lr=0.01, momentum=0.95, nesterov=True)\n",
    "\n",
    "numObs =  15 \n",
    "obs_x = np.sort((np.random.choice(x, numObs, replace=False)))\n",
    "obs_t = f(obs_x)\n",
    "test_x = np.linspace(-30, 85, 1)\n",
    "\n",
    "nlog_lik_steps = []\n",
    "for i in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    params = [logl.exp(), logsigmaf.exp(), logsigman.exp()]\n",
    "    _, _, log_lik = gp_inference(torch.from_numpy(obs_x).float(), torch.from_numpy(obs_t).float(), torch.from_numpy(test_x).float(), params) \n",
    "    # Instead for maximize the log-likelihood, we minimize the negative log likelihood\n",
    "    nlog_lik = -log_lik\n",
    "    nlog_lik.backward()\n",
    "    optimizer.step()\n",
    "    nlog_lik_steps.append(nlog_lik)\n",
    "    \n",
    "print('NLOG_LIK = %.2f, LENGHTSCALE = %.2f, SIGMA_F = %.2f, SIGMA_N = %.2f' % (nlog_lik.item(), logl.exp(), logsigmaf.exp(), logsigman.exp()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did it converge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[8, 3])\n",
    "ax.plot(range(len(nlog_lik_steps)), nlog_lik_steps)\n",
    "ax.set_ylabel(r'$p(Y|X, \\theta^t)$')\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_title('Optimization of the marginal likelihood w.r.t the kernel parameters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see where and how it converged!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [logl.exp(), logsigmaf.exp(), logsigman.exp()]\n",
    "test_x = np.linspace(-30, 85, 1000)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(obs_x,obs_t, '.', c='black', label='Observations')\n",
    "\n",
    "posterior_m, posterior_v, log_lik = gp_inference(torch.tensor(obs_x, dtype=torch.float), \n",
    "                                                 torch.tensor(obs_t, dtype=torch.float), \n",
    "                                                 torch.tensor(test_x, dtype=torch.float), params) \n",
    "\n",
    "posterior_m = posterior_m.detach()\n",
    "posterior_v = posterior_v.detach()\n",
    "posterior_std = np.sqrt(np.diag(posterior_v))\n",
    "\n",
    "ax.plot(np.linspace(-30, 85, 1000), posterior_m.numpy(), color=\"grey\", label=r'posterior $\\mu$')  \n",
    "\n",
    "ax.fill_between(np.linspace(-30, 85, 1000), posterior_m.numpy() + posterior_std*2,posterior_m.numpy() - posterior_std*2,\n",
    "                color=\"grey\", alpha=0.2, label=r'posterior $2\\sigma$')\n",
    "ax.set_title(r'$l=$%.2f, $\\sigma_f^2=$%.2f, $\\sigma_n^2=$%.2f' % (params[0], params[1], params[2]))\n",
    "\n",
    "ax.text(80, 1.25, r'$p(Y|X, \\theta)$ = %.2f' % log_lik, horizontalalignment='right')\n",
    "ax.legend()\n",
    "ax.set_ylim(-1.5, 1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! During optimization of the marginal likelihood, the model recovered that observations were noisyless. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it, folks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Maybe some links here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
