{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1 style=\"text-align:center\">Modern Gaussian Processes: Scalable Inference and Novel Applications (IJCNN '19)</h1>\n",
    "<h2 style=\"text-align:center\">Tutorial: Inference of Gaussian Process Regression models</h2>\n",
    "\n",
    "- **Edwin V. Bonilla**, Data61, Australia \n",
    "- **Maurizio Filippone**, EURECOM, France"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Aims\n",
    "<div class=\"alert alert-info\">\n",
    "<ul> \n",
    "<li> To implement Gaussian process inference for regression.\n",
    "<li> To use the above to observe samples from a Gaussian process posterior distribution.\n",
    "<li> To evaluate how different hyperparameter settings impact model quality.\n",
    "<li> To investigate different kernel functions and parameter optimisation strategies.\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "**Note**: we shall use PyTorch for reasons that will be clear later on in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "if int(re.search(r'([\\d.]+)', torch.__version__).group(1).replace('.', '')) < 100:\n",
    "    raise ImportError('Your PyTorch version is not supported. Please download and install PyTorch 1.x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import warnings\n",
    "matplotlib.rc_file('matplotlibrc')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Introduction\n",
    "<div class=\"alert alert-info\">\n",
    "In this tutorial, we shall cover the basic concepts of <b>GP regression</b>. For the sake of clarity, we shall focus on univariate data, which allows for better visualisation of the GP model. Nonetheless, the code implemented within this lab can be very easily extended to handle\n",
    "multi-dimensional inputs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "As in the previous notebook, we shall consider a one-dimensional regression problem, whereby the inputs x are transformed by\n",
    "a function \n",
    "<br><br>\n",
    "    $$ f(\\mathbf{x}) = sin(exp(0.03 * \\mathbf{x}))\\,.$$\n",
    "<br>\n",
    "Let's generate 500 random points, $x$, in the range $[-20, 80]$, and compute their corresponding function\n",
    "values, $y$ (assuming noisyless observations for the moment). The target function can then be plotted accordingly.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "set_seed()\n",
    "matplotlib.rc_file('matplotlibrc')\n",
    "n = 100\n",
    "\n",
    "# Define our function\n",
    "f = lambda x: np.sin(np.exp(.03 * x))\n",
    "# Define our observation points\n",
    "x = np.sort(np.random.uniform(-20,80,n))\n",
    "# Define our target points\n",
    "y = f(x)\n",
    "\n",
    "# Plot \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y, '.', c='black', label='Observations')\n",
    "ax.plot(np.linspace(-30, 85, 1000), f(np.linspace(-30, 85, 1000)), alpha=0.6, label=r\"$f({x}) = sin(e^{0.03 {x}})$\" )\n",
    "ax.set_title(\"Regression problem\")\n",
    "ax.set_ylim(-1.5, 1.5)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Recall that GPs are non-parametric model. We define a prior distribution over functions (models),\n",
    "specified as a multivariate Gaussian distribution $p(f) = N (\\mu, \\Sigma)$.\n",
    "\n",
    "Without loss of generality, we shall assume a zero-mean GP prior, i.e. $\\mu = 0$. The covariance\n",
    "matrix of the distribution, $\\Sigma$, may then be computed by evaluating the covariance between the\n",
    "input points. For this tutorial, we consider the widely used squared-exponential (RBF)\n",
    "covariance.\n",
    "\n",
    "As a reminder, the RBF kernel is defined between two points as: \n",
    "\n",
    "$$k(x, x') = \\sigma_f^2 \\exp \\Big( -\\dfrac {(x-x')^2}{2l^2} \\Big). $$\n",
    "\n",
    "This kernel is parameterised by a lengthscale parameter $l$, and variance $\\sigma_f^2$ . Given that the true\n",
    "function may be assumed to be corrupted with noise, we can also add a noise parameter, $\\sigma_n^2$ , to\n",
    "the diagonal entries of the resulting kernel matrix, $K$, such that\n",
    "$$K_y = K + \\sigma_n^2I.$$\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def cdist(x1, x2):\n",
    "    \"\"\"\n",
    "    Compute distance between each pair of the two collections of input tensors.\n",
    "    see scipy.spatial.distance.cdist\n",
    "    \"\"\"\n",
    "    x1_norm = x1.pow(2).sum(dim=-1, keepdim=True)\n",
    "    x2_norm = x2.pow(2).sum(dim=-1, keepdim=True)\n",
    "    res = torch.addmm(x2_norm.transpose(-2, -1), x1, x2.transpose(-2, -1), alpha=-2).add_(x1_norm)\n",
    "    res = res.clamp_min_(1e-30).sqrt_()\n",
    "    return res\n",
    "\n",
    "def rbf_kernel(x1, x2, lengthscale, variance):\n",
    "    \"\"\"\n",
    "    Compute the RBF covariance matrix \n",
    "    \"\"\"\n",
    "    K = variance * torch.exp(-cdist(x1[...,None], x2[...,None])**2 / (2 * lengthscale**2))\n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Assuming a zero-mean prior, and using the kernel matrix constructed with `rbf_kernel()` for input points x, we can sample from the prior distribution using the numpy `multivariate_normal()` function.\n",
    "<br><br>\n",
    "For the time being, you can initialise the kernel parameters as follows:\n",
    "<br>\n",
    "- lengthscale = 10<br>\n",
    "- variance = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "set_seed()\n",
    "# Define points \n",
    "test_x = torch.from_numpy(np.linspace(-30, 85, 200)).float()\n",
    "\n",
    "# Define kernel parameters\n",
    "lengthscale = 10\n",
    "variance = .1\n",
    "\n",
    "# Sample from GP prior\n",
    "mu = np.zeros(len(test_x))\n",
    "K = rbf_kernel(test_x, test_x, lengthscale, variance)\n",
    "samples = 30\n",
    "f_i = np.random.multivariate_normal(mu, K, samples)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(samples):\n",
    "    ax.plot(test_x.numpy(), f_i[i,:], c='#ff7f00', alpha=0.5)\n",
    "\n",
    "ax.plot(test_x.numpy(), mu, color=\"grey\", label=r'prior $\\mu$')   \n",
    "ax.fill_between(test_x.numpy(),mu + np.sqrt(variance) * 2, mu - np.sqrt(variance) * 2,\n",
    "                color=\"grey\", alpha=0.2, label=r'prior $2\\sigma\\approx95\\%\\,CI$')\n",
    "ax.set_title('Sampling from the GP prior')\n",
    "ax.set_ylim(-1.5, 1.5)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3. GP Inference\n",
    "<div class=\"alert alert-info\">\n",
    "Suppose we can now observe 3 points at random from the input data; we would expect that with this additional knowledge, the functions drawn from the updated GP distribution would be constrained to pass through these points (or at least close if corrupted with noise). The combination of the prior and the likelihood of the observed data leads to the posterior distribution over functions.\n",
    "<br><br>\n",
    "Assign 3 points at random from $x$ (and their corresponding function values) to `obs_x` and `obs_t`\n",
    "respectively. For now we shall assume that all other $x$ values are unobserved.<br><br>\n",
    "\n",
    "You are encouraged to use the given initial configuration.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "A complete implementation of `gp_inference` is provided for evaluating the posterior GP mean and variance using the equations given in the tutorial.\n",
    "<br><br>\n",
    "<b>Note</b>: Matrix inversions can be both numerically troublesome and slow to compute. In this notebook, we shall avoid computing matrix inversions directly by instead considering Cholesky decompositions for solving linear systems. You are encouraged to read more about Cholesky decompositions for GPs by consulting Appendix A.4 of <a target=\"_blank\" href=\"http://www.gaussianprocess.org/gpml/\">Gaussian Processes for Machine Learning (Rasmussen and Williams, 2005)</a> - available online!<br><br>\n",
    "The complete pseudo-code for the following procedure is provided in Algorithm 2.1 from Chapter 2 of this same book.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gp_inference(obs_x, obs_t, x_new, params, prior_mean=0):\n",
    "    \n",
    "    # unpack params\n",
    "    lengthscale = params[0]\n",
    "    variance = params[1]\n",
    "    noise = params[2]\n",
    "    obs_t = obs_t - prior_mean\n",
    "    \n",
    "    N = obs_x.shape[0]\n",
    "    \n",
    "    # compute kernel\n",
    "    K = rbf_kernel(obs_x, obs_x, lengthscale,variance)\n",
    "    K_y = K + noise * torch.eye(obs_x.shape[0])\n",
    "     \n",
    "    '''\n",
    "    When computing the posterior mean, we would like to avoid evaluating\n",
    "        \n",
    "                            alpha = (K_y)^-1 * obs_t\n",
    "    \n",
    "    directly. The Cholesky decomposition can be applied using the following procedure.\n",
    "    \n",
    "        -> Compute the lower triangular Cholesky decomposition of K_y (which we shall call K_chol)\n",
    "        -> Compute 'alpha' as:\n",
    "        \n",
    "                            alpha = K_chol.T \\ (K_chol \\ obs_t)\n",
    "                            \n",
    "           where the back-substitution operator can be evaluated using the 'trtrs' (solve_triangular) \n",
    "           function in pytorch. Make sure to set the function's upper' flag as appropriate.\n",
    "    '''  \n",
    "    # compute the Cholesky decomposition of K_y\n",
    "    K_chol = torch.cholesky(K_y)\n",
    "    \n",
    "    # compute alpha\n",
    "    alpha = torch.trtrs(torch.trtrs(obs_t, K_chol, upper=False)[0], K_chol.t(), upper=True)[0]\n",
    "    \n",
    "    # compute the covariance between the training and test data\n",
    "    K_obs_pred = rbf_kernel(obs_x, x_new, lengthscale, variance)\n",
    "    \n",
    "    # compute the covariance for the test data\n",
    "    K_pred = rbf_kernel(x_new, x_new, lengthscale, variance)\n",
    "    \n",
    "    # compute the posterior mean\n",
    "    posterior_m = torch.matmul(K_obs_pred.t(), alpha)\n",
    "    \n",
    "    '''\n",
    "    Similarly, when computing\n",
    "    \n",
    "                        v = (kern_obs)^-1 * kern_obs_pred\n",
    "                        \n",
    "    employ the Cholesky decomposition as outlined above.                 \n",
    "    '''\n",
    "    # compute the posterior variance\n",
    "    v = torch.trtrs(torch.trtrs(K_obs_pred, K_chol,upper=False)[0], K_chol.t(),upper=True)[0]\n",
    "    posterior_v =  K_pred - torch.matmul(K_obs_pred.t(), v)\n",
    "    \n",
    "    # compute the marginal log-likelihood\n",
    "    log_lik = -.5 * (torch.sum(torch.log(torch.abs(torch.diag(K_chol)))) +\n",
    "                     torch.sum(torch.trtrs((obs_t), K_chol,upper=False)[0]**2)) - N/2. * np.log(2 * np.pi)\n",
    "    \n",
    "    return posterior_m[...,0] + prior_mean, posterior_v, log_lik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the GP inference. For plot convenience, let's take 1000 points on the real axis as $x_\\mathrm{new}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "# Define observed points\n",
    "numObs = 3\n",
    "obs_x = torch.from_numpy(np.sort((np.random.choice(x, numObs, replace=False)))).float()\n",
    "obs_t = f(obs_x)\n",
    "\n",
    "# Define kernel parameters\n",
    "lengthscale = 10\n",
    "variance = .1\n",
    "noise = 1e-5\n",
    "params = [lengthscale, variance, noise]\n",
    "\n",
    "# Run inference and get posterior mean and variance\n",
    "posterior_m, posterior_v, log_lik = gp_inference(obs_x, obs_t, test_x, params)\n",
    "posterior_std = torch.sqrt(torch.diag(posterior_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4. Sampling from the  GP Posterior\n",
    "<br>\n",
    "<div class=\"alert alert-info\">\n",
    "Now that you have computed the posterior mean and variance, let's create a figure showing the true function. To this figure, we add the posterior mean and uncertainty (show two standard deviations) evaluated on the same $x$ values. Remember that the variance at each point is given by the diagonal of the covariance matrix.\n",
    "    Let's also plot 10 samples from the posterior GP after inference.\n",
    "<!-- Recall that the standard deviation is the square root of the variance. -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "# Samples from the GP (posterior in this case)\n",
    "samples = 10\n",
    "f_i = np.random.multivariate_normal(posterior_m, posterior_v, samples) \n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(test_x.numpy(), f(test_x).numpy(), alpha=0.75, label=r\"$f({x}) = sin(e^{0.03 {x}})$\" )\n",
    "\n",
    "for i in range(samples):\n",
    "    ax.plot(test_x.numpy(),f_i[i,:], c=\"#ff7f00\", alpha=0.5)\n",
    "ax.plot(obs_x.numpy(), obs_t.numpy(), '.', c='black', label='Observations')\n",
    "\n",
    "ax.plot(test_x.numpy(), posterior_m.numpy(), color=\"grey\", label=r'posterior $\\mu$')  \n",
    "\n",
    "ax.fill_between(test_x.numpy(), (posterior_m + posterior_std * 2).numpy(), (posterior_m - posterior_std * 2).numpy(),\n",
    "                color=\"grey\", alpha=0.2, label=r'posterior $2\\sigma\\approx95\\%\\,CI$')\n",
    "\n",
    "ax.set_title('Sampling from the GP posterior')\n",
    "ax.legend()\n",
    "ax.set_ylim(-1.5, 1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Try to increase the number of observations. What do you see?\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "fig, ((ax0, ax1), (ax2, ax3)) = plt.subplots(2, 2, sharex=True, sharey=True)\n",
    "\n",
    "# Just a simple function to make it less verbose\n",
    "def run_inference(numObs, ax):\n",
    "    obs_x = torch.from_numpy(np.sort((np.random.choice(x, numObs, replace=False)))).float()\n",
    "    obs_t = f(obs_x)\n",
    "\n",
    "    posterior_m, posterior_v, log_lik = gp_inference(obs_x, obs_t, test_x, params) \n",
    "    posterior_std = torch.sqrt(torch.diag(posterior_v))\n",
    "    \n",
    "    ax.plot(obs_x.numpy(), obs_t.numpy(), '.', c='black', label='Observations')\n",
    "    ax.plot(test_x.numpy(), posterior_m.numpy(), color=\"grey\", label=r'posterior $\\mu$')  \n",
    "    ax.fill_between(test_x.numpy(), (posterior_m + posterior_std * 2).numpy(), (posterior_m - posterior_std * 2).numpy(),\n",
    "                    color=\"grey\", alpha=0.2, label=r'posterior $2\\sigma\\approx95\\%\\,CI$')\n",
    "  \n",
    "    ax.set_title('%d observations' % numObs)    \n",
    "    ax.set_ylim(-1.5, 1.5)\n",
    "    \n",
    "    \n",
    "run_inference(4, ax0)\n",
    "run_inference(10, ax1)\n",
    "run_inference(15, ax2)\n",
    "run_inference(35, ax3)\n",
    "\n",
    "ax3.legend(loc='lower left')\n",
    "fig.suptitle('GP inference with increasing training points', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 4.1 Model evaluation</h2>\n",
    "<div class=\"alert alert-info\">\n",
    "Try to change the GP prior (for instance the lenghtscale of the RBF kernel) and run again the GP inference. What do you see?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "# Define points\n",
    "numObs =  15 \n",
    "obs_x = torch.from_numpy(np.sort((np.random.choice(x, numObs, replace=False)))).float()\n",
    "obs_t = f(obs_x)\n",
    "\n",
    "# Just a simple function to make it less verbose\n",
    "def run_inference(params, ax):\n",
    "    posterior_m, posterior_v, log_lik = gp_inference(obs_x, obs_t, test_x, params) \n",
    "    posterior_std = torch.sqrt(torch.diag(posterior_v))\n",
    "    \n",
    "    ax.plot(obs_x.numpy(), obs_t.numpy(), '.', c='black', label='Observations')\n",
    "    ax.plot(test_x.numpy(), posterior_m.numpy(), color=\"grey\", label=r'posterior $\\mu$')  \n",
    "    ax.fill_between(test_x.numpy(), (posterior_m + posterior_std * 2).numpy(), (posterior_m - posterior_std * 2).numpy(),\n",
    "                    color=\"grey\", alpha=0.2, label=r'posterior $2\\sigma\\approx95\\%\\,CI$')\n",
    "    ax.set_title(r'$l=$%.0f, $\\sigma_f^2=$%.2f' % (params[0], params[1]))\n",
    "    ax.text(80, 1.1, r'$p(Y|X, \\theta)$ = %.2f' % log_lik, horizontalalignment='right')\n",
    "    ax.set_ylim(-1.5, 1.5)\n",
    "\n",
    "fig, ((ax0, ax1), (ax2, ax3)) = plt.subplots(2, 2, sharex=True, sharey=True)\n",
    "\n",
    "# params = [lengthscale, variance, noise]\n",
    "params = [3, 0.01, 1e-2]\n",
    "run_inference(params, ax0)\n",
    "params = [3, 0.1, 1e-2]\n",
    "run_inference(params, ax1)\n",
    "params = [10, 0.01, 1e-2]\n",
    "run_inference(params, ax2)\n",
    "params = [10, 0.1, 1e-2]\n",
    "run_inference(params, ax3)\n",
    "\n",
    "ax3.legend(loc='lower left')\n",
    "fig.suptitle('Inference with different GP prior', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "As a measure of model quality, you should check the log marginal likelihood of the model (the higher, the better).\n",
    "<!-- To this end, complete the code provided in `gp inference()` to include the negative log likelihood term. -->\n",
    "<br><br>\n",
    "You could attempt a grid search over a range of parameter values in order to determine which configuration yields the best result<br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "# Define points\n",
    "numObs =  15 \n",
    "obs_x = torch.from_numpy(np.sort((np.random.choice(x, numObs, replace=False)))).float()\n",
    "obs_t = f(obs_x)\n",
    "\n",
    "# Plot the likelihood surface\n",
    "l_points = np.linspace(7, 10, 50)\n",
    "logsigmaf_points = np.logspace(-2, 0, 50, base=10)\n",
    "log_lik_points = np.zeros([len(l_points), len(logsigmaf_points)])\n",
    "\n",
    "for i, logl in enumerate(l_points):\n",
    "    for j, logsigmaf in enumerate(logsigmaf_points):\n",
    "        posterior_m, posterior_v, log_lik = gp_inference(obs_x, obs_t, \n",
    "                                                         torch.from_numpy(np.linspace(-30, 85, 1)).float(),\n",
    "                                                         [logl, 10. ** logsigmaf, 1e-5]) \n",
    "        log_lik_points[i, 49-j] = -log_lik\n",
    "    \n",
    "fig, ax = plt.subplots()   \n",
    "cp = ax.contour(logsigmaf_points, l_points, log_lik_points, levels=np.logspace(np.log(6.5), np.log(9), 30, base=np.e))\n",
    "cb = plt.colorbar(cp)\n",
    "cb.ax.set_title(r\"$p(Y|X, \\theta)$\",)   \n",
    "best_coordinates = np.unravel_index(log_lik_points.argmin(), log_lik_points.shape)\n",
    "ax.plot(logsigmaf_points[best_coordinates[1]], l_points[best_coordinates[0]], '*', markersize=16, label='Best hyperparameters')\n",
    "ax.set_ylabel(r'lenghtscale l')\n",
    "ax.set_xlabel(r'signal variance $\\sigma_f^2$')\n",
    "ax.legend(loc='bottom left')\n",
    "ax.set_title('Optimization landscape\\nof the negative marginal likelihood')\n",
    "ax.semilogx()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But can we do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 5. Parameter Optimisation using Gradient Descent\n",
    "<br>\n",
    "<div class=\"alert alert-info\">\n",
    "Optimise the hyperparameters of the model by minimising the negative log-likelihood of the model. For a complete solution, you should include the derivatives of the objective function with respect to the parameters being optimised.\n",
    "<br><br>\n",
    "The general formula for computing the derivative is given below:<br>\n",
    "$$\n",
    "\\frac{\\partial\\;\\text{NLL}}{\\partial\\;\\theta_i} = - \\frac{1}{2} \\textbf{Tr} \\left( K^{-1} \\frac{\\partial K}{\\partial \\theta_i} \\right) + \\frac{1}{2} \\textbf{y}^{T} K^{-1} \\frac{\\partial K}{\\partial \\theta_i} K^{-1} \\textbf{y}.\n",
    "$$<br>\n",
    "To give a more concrete example, the $\\frac{\\partial K}{\\partial \\theta_i}$ term for the lengthscale parameter in the RBF kernel is computed as follows:\n",
    "$$\n",
    "\\frac{\\partial K}{\\partial l} = \\sigma_f^2 \\exp \\left( -\\dfrac {(x-x')^2}{2l^2} \\right)\\left( \\dfrac {(x-x')^2}{l^3} \\right)\n",
    "$$\n",
    "<br><br>\n",
    "<b>Pro tip:</b> Note that the parameters $l$, $\\sigma_f^2$ , and $\\sigma_n^2$ are always expected to be positive. It is possible that the optimisation algorithm attempts to evaluate the log-likelihood in regions of the parameter space where one or more of these parameters are negative, leading to numerical issues. A commonly-used technique to enforce this condition is to work with a transformed version of covariance parameters using the logarithm transformation. In particular, define $\\psi_l = log(l)$, $\\psi_f = log(\\sigma_f^2 )$, and $\\psi_n = log(\\sigma_n^2 )$, and optimise with respect to the $\\psi$ parameters. The optimisation problem in the transformed space is now unbounded, and the gradient of the log-likelihood should be computed with respect to the $\\psi$ parameters.\n",
    "<br><br>\n",
    "<b>Pro tip 2019:</b>\n",
    "We don't really need to derive the gradients of the marginal likelihood w.r.t. parameters by hand. We can leverage the automatic differentiation engine in PyTorch! All the operations that we used (cdist and the RBF kernel function, the Cholesky decomposition and triangular linear system solver are all differentiable). Let's take advantage of that!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "# Transformed covariance parameters\n",
    "logl = torch.nn.Parameter(torch.tensor(2).float())\n",
    "logsigmaf = torch.nn.Parameter(torch.tensor(-2).float())\n",
    "logsigman = torch.nn.Parameter(torch.tensor(-2).float())\n",
    "\n",
    "# Just like vanilla PyTorch for DL\n",
    "optimizer = torch.optim.SGD([logl, logsigmaf, logsigman], lr=0.01, momentum=0.95, nesterov=True)\n",
    "\n",
    "# Define points\n",
    "numObs =  15 \n",
    "obs_x = torch.from_numpy(np.sort((np.random.choice(x, numObs, replace=False)))).float()\n",
    "obs_t = f(obs_x)\n",
    "\n",
    "# Run training loop\n",
    "nlog_lik_steps = []\n",
    "for i in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    params = [logl.exp(), logsigmaf.exp(), logsigman.exp()]\n",
    "    _, _, log_lik = gp_inference(obs_x, obs_t, torch.from_numpy(np.linspace(-30, 85, 1)).float(), params) \n",
    "    \n",
    "    # Instead for maximize the log-likelihood, we minimize the negative log likelihood\n",
    "    nlog_lik = -log_lik\n",
    "    nlog_lik.backward()\n",
    "    optimizer.step()\n",
    "    nlog_lik_steps.append(nlog_lik)\n",
    "    \n",
    "print('NLOG_LIK = %.2f, LENGHTSCALE = %.2f, SIGMA_F = %.2f, SIGMA_N = %.2f' % (nlog_lik.item(), logl.exp(), logsigmaf.exp(), logsigman.exp()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did it converge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[8, 3])\n",
    "ax.plot(range(len(nlog_lik_steps)), nlog_lik_steps)\n",
    "ax.set_ylabel(r'$p(Y|X, \\theta^t)$')\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_title('Optimization of the marginal likelihood w.r.t the kernel parameters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see where and how it converged!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [logl.exp(), logsigmaf.exp(), logsigman.exp()]\n",
    "\n",
    "# Run inference and ...\n",
    "posterior_m, posterior_v, log_lik = gp_inference(obs_x, obs_t, test_x, params) \n",
    "posterior_m = posterior_m.detach()\n",
    "posterior_v = posterior_v.detach()\n",
    "posterior_std = torch.sqrt(torch.diag(posterior_v))\n",
    "\n",
    "# ... and plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(obs_x.numpy(), obs_t.numpy(), '.', c='black', label='Observations')\n",
    "ax.plot(test_x.numpy(), posterior_m.numpy(), color=\"grey\", label=r'posterior $\\mu$')  \n",
    "ax.fill_between(test_x.numpy(), (posterior_m + posterior_std * 2).numpy(), (posterior_m - posterior_std * 2).numpy(),\n",
    "                    color=\"grey\", alpha=0.2, label=r'posterior $2\\sigma\\approx95\\%\\,CI$')\n",
    "ax.set_title(r'$l=$%.2f, $\\sigma_f^2=$%.2f, $\\sigma_n^2=$%.2f' % (params[0], params[1], params[2]))\n",
    "ax.text(80, 1.25, r'$p(Y|X, \\theta)$ = %.2f' % log_lik, horizontalalignment='right')\n",
    "ax.legend()\n",
    "ax.set_ylim(-1.5, 1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! During optimization of the marginal likelihood, the model recovered that observations were noisyless. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it, folks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Regression on Classification labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification using non-Gaussian likelihood makes inference intractable. \n",
    "One could think to solve this issue by running GP regression directly on classification labels. \n",
    "In this case each point would be associated with a Gaussian likelihood, which is not the appropriate noise model for Bernoulli-distributed variables. \n",
    "Recently it has been shown how to transform the labels in a latent space where a Gaussian approximation to the likelihood is more sensible (Milios et al., 2018).\n",
    "\n",
    "### The boring details\n",
    "The transformation of the labels is based on the formalization of a simple intuition, which is the inversion of the softmax transformation. \n",
    "Labels are viewed as a set of parameters of a degenerate Dirichlet distribution. \n",
    "The degeneracy of the Dirichlet distribution is solved by adding a small regularization, say $\\alpha = 0.05$, to the parameters.\n",
    "At this point, Dirichlet distributed random variables can be constructed as a ratio of Gamma random variables, that is, if $x_i \\sim \\mathrm{Gamma}(a_i, b)$, then $\\frac{x_i}{\\sum_j x_j}\\sim \\mathrm{Dir}(\\mathbf{a})$.\n",
    "\n",
    "The Gamma random variables are approximated with log-Normals by moment matching, which become Gaussian after a logarithm transformation. \n",
    "By doing so, we obtain a representation of the labels which allows us to use standard regression with a Gaussian likelihood, and which retrieves an approximate Dirichlet when mapping predictions back using the softmax transformation. \n",
    "As a result, the latent functions obtained represent probabilities of class labels.\n",
    "The only small complication is that the transformation imposes a different noise level for labels that are $0$ or $1$, yielding a heteroskedastic regression model.\n",
    "\n",
    "### In practice\n",
    "Given $Y = \\{y_i\\, |\\,  y_i = f(x_i)\\}$ where $y_i$ is one-hot encoded, the transformation is defined as:\n",
    "$$\n",
    "\\text{var}(Y) = \\log{[(Y + \\alpha)^{-1} + 1]}\n",
    "$$\n",
    "$$\n",
    "\\text{mean}(Y) = \\log{(Y + \\alpha) - \\text{var}(Y)/2}\n",
    "$$\n",
    "Given this approximation, we place a GP prior over $\\mathbf{f}$ and evaluate the posterior over the $C$ latent processes (where $C$ is the number of classes). \n",
    "It is possible to make kernel parameters independent across processes, or shared so that they are informed by all classes. \n",
    "For simplicity we will share the kernel parameters but you can try to make them independed. \n",
    "\n",
    "Let's run a simple experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "# Define dataset\n",
    "numObs = 25\n",
    "obs_x = torch.from_numpy(np.sort((np.random.choice(x, numObs, replace=False)))).float()\n",
    "obs_t = torch.from_numpy(np.stack([np.where(f(obs_x)>=0, 1, 0), np.where(f(obs_x)>=0, 0, 1)]).T).float()\n",
    "test_x = torch.from_numpy(np.linspace(-30, 85, 1)).float()\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(1, 2, figsize=[5, 2], sharey=True)\n",
    "ax0.plot(obs_x.numpy(), obs_t.numpy()[:,0], '.', color='black', label='Class 0')\n",
    "ax1.plot(obs_x.numpy(), obs_t.numpy()[:,1], '.', color='black', label='Class 1')\n",
    "ax0.set_ylim(-0.05, 1.05)\n",
    "ax1.set_ylim(-0.05, 1.05)\n",
    "ax0.legend(loc='lower left')\n",
    "ax1.legend()\n",
    "fig.suptitle('Classification problem', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "# Define parameters to optimize\n",
    "logl = torch.nn.Parameter(torch.tensor(1).float())\n",
    "logsigmaf = torch.nn.Parameter(torch.tensor(-1).float())\n",
    "\n",
    "# Just like vanilla PyTorch for DL\n",
    "optimizer = torch.optim.SGD([logl, logsigmaf, ], lr=0.01, momentum=0.95, nesterov=True)\n",
    "\n",
    "# Computes Dirichlet transformed variables\n",
    "alpha = 0.05\n",
    "obs_t_transf_var = torch.log((obs_t + alpha) ** (-1) + 1 )\n",
    "obs_t_transf_mean = torch.log(obs_t + alpha) - (obs_t_transf_var) / 2\n",
    "\n",
    "# Run optimization\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    params = [logl.exp(), logsigmaf.exp()]\n",
    "    _, _, log_lik_0 =  gp_inference(obs_x, obs_t_transf_mean[:,0], test_x, params + [obs_t_transf_var[:,0]]) \n",
    "    _, _, log_lik_1 =  gp_inference(obs_x, obs_t_transf_mean[:,1], test_x, params + [obs_t_transf_var[:,1]]) \n",
    "    nlog_lik = - log_lik_0 - log_lik_1\n",
    "    nlog_lik.backward()\n",
    "    optimizer.step()   \n",
    "\n",
    "# Run final inference and ...\n",
    "test_x = torch.from_numpy(np.linspace(-50, 115, 200)).float()\n",
    "prior_mean = (obs_t_transf_mean.max() + obs_t_transf_mean.min())/2\n",
    "params = [logl.exp(), logsigmaf.exp()]\n",
    "posterior_m_0, posterior_v_0, log_lik = gp_inference(obs_x, obs_t_transf_mean[:,0], test_x, params + [obs_t_transf_var[:,0]], prior_mean) \n",
    "posterior_m_1, posterior_v_1, log_lik = gp_inference(obs_x, obs_t_transf_mean[:,1], test_x, params + [obs_t_transf_var[:,1]], prior_mean) \n",
    "posterior_m = torch.stack([posterior_m_0.detach(), posterior_m_1.detach()]).t()\n",
    "posterior_std_0 = torch.sqrt(torch.diag(posterior_v_0.detach()))\n",
    "posterior_std_1 = torch.sqrt(torch.diag(posterior_v_1.detach()))\n",
    "posterior_std = torch.stack([posterior_std_0.detach(), posterior_std_1.detach()]).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "# ... and plot\n",
    "q = 95\n",
    "mean_true = np.zeros([posterior_m.shape[0], 2])\n",
    "lb_true = np.zeros([posterior_m.shape[0], 2])\n",
    "ub_true = np.zeros([posterior_m.shape[0], 2])\n",
    "source = np.random.randn(1000, 2)\n",
    "for i in range(posterior_m.shape[0]):\n",
    "    samples = source * np.sqrt(posterior_std[i,:].numpy()) + posterior_m[i,:].numpy()\n",
    "    samples = np.exp(samples) / np.exp(samples).sum(1).reshape(-1, 1)\n",
    "    Q = np.percentile(samples, [100-q, q], axis=0)\n",
    "    mean_true[i,:] = samples.mean(0)\n",
    "    lb_true[i,:] = Q[0,:]\n",
    "    ub_true[i,:] = Q[1,:]\n",
    "    \n",
    "fig, ((ax0, ax1), (ax2, ax3)) = plt.subplots(2, 2, figsize=[9, 5], sharex=True)\n",
    "\n",
    "ax0.plot(obs_x.numpy(), obs_t_transf_mean[:,0].numpy(), '.', c='black', label='Observations')\n",
    "ax0.plot(test_x.numpy(), posterior_m[:,0].numpy(), color=\"grey\", label=r'posterior $\\mu$')  \n",
    "ax0.fill_between(test_x.numpy(), (posterior_m[:,0] + posterior_std_0*2).numpy(), (posterior_m[:,0] - posterior_std_0*2).numpy(),\n",
    "                color=\"grey\", alpha=0.2, label=r'posterior $2\\sigma$')\n",
    "ax0.errorbar(obs_x.numpy(), obs_t_transf_mean[:,0].numpy(), obs_t_transf_var[:,0].sqrt().numpy(),  fmt='.', c='black', alpha=.37)\n",
    "\n",
    "ax1.plot(obs_x.numpy(), obs_t[:,0].numpy(), '.', c='black', label='Observations')\n",
    "ax1.plot(test_x.numpy(), mean_true[:,0], color=\"grey\", label=r'posterior $\\mu$')  \n",
    "ax1.fill_between(test_x.numpy(), lb_true[:,0], ub_true[:,0],\n",
    "                color=\"grey\", alpha=0.2, label=r'posterior $2\\sigma$')\n",
    "\n",
    "ax2.plot(obs_x.numpy(), obs_t_transf_mean[:,1].numpy(), '.', c='black', label='Observations')\n",
    "ax2.plot(test_x.numpy(), posterior_m[:,1].numpy(), color=\"grey\", label=r'posterior $\\mu$')  \n",
    "ax2.fill_between(test_x.numpy(), (posterior_m[:,1] + posterior_std_1*2).numpy(), (posterior_m[:,1] - posterior_std_1*2).numpy(),\n",
    "                color=\"grey\", alpha=0.2, label=r'posterior $2\\sigma$')\n",
    "ax2.errorbar(obs_x.numpy(), obs_t_transf_mean[:,1].numpy(), obs_t_transf_var[:,1].sqrt().numpy(),  fmt='.', c='black', alpha=.37)\n",
    "\n",
    "ax3.plot(obs_x.numpy(), obs_t[:,1].numpy(), '.', c='black', label='Observations')\n",
    "ax3.plot(test_x.numpy(), mean_true[:,1], color=\"grey\", label=r'posterior $\\mu$')  \n",
    "ax3.fill_between(test_x.numpy(), lb_true[:,1], ub_true[:,1],\n",
    "                color=\"grey\", alpha=0.2, label=r'posterior $2\\sigma$')\n",
    "\n",
    "ax0.set_title('GP in latent space (class 0)')\n",
    "ax0.set_ylim(-8,2)\n",
    "ax1.set_title('GP in original space (class 0)')\n",
    "ax1.set_ylim(-0.05, 1.05)\n",
    "ax2.set_title('GP in latent space (class 1)')\n",
    "ax2.set_ylim(-8,2)\n",
    "ax3.set_title('GP in latent space (class 1)')\n",
    "ax3.set_ylim(-0.05, 1.05)\n",
    "ax3.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
